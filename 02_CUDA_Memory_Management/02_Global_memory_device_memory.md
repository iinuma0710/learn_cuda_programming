# 2.2 Global memory / device memory
この節では，グローバルメモリとデバイスメモリの詳しい使い方について見ていく．
また，グローバルメモリからキャッシュに効果的にデータをロード，ストアする方法についても見ていく．
グローバルメモリは CPU メモリから全てのデータをステージングする場所なので，
うまくこのメモリを使いこなすのは非常に重要である．
グローバルメモリとデバイスメモリは，カーネル内のスレッド全てから参照できる．
また，CPU からも参照可能である．

プログラマは ```cudaMalloc``` と ```cudaFree``` を使って割当と解放を明示的に行う必要がある．
データは ```cudaMalloc``` で割り当てられ，```__device__``` として宣言される．
グローバルメモリはデフォルトで ```cudaMemcpy``` API を使って CPU から送られてきた
全てのデータをステージングする場所である．


## 2.2.1 Vector addition on global memory
ベクトル加算のレイは最初の章でも扱ったが，ここではグローバルメモリの使い方について見ていく．
次のコードから，どのようにグローバルメモリを使っているのか見てみよう．

```c
__global__ void device_add(int *a, int *b, int *c)
{
    int index = threadIdx.x + blockIdx.x * blockDim.x;
    c[index] = a[index] + b[index];
}

int main(void)
{
    ...

    // a, b, c に対応する GPU メモリを確保
    cudaMalloc((void **)&d_a, size);
    cudaMalloc((void **)&d_b, size);
    cudaMalloc((void **)&d_c, size);

    ...

    // GPU メモリの解放
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    ...
}
```

```cudaMalloc``` はデバイスメモリ上のデータ領域を割り当てる．
カーネルのポインタ引数 a，b，c は，このデバイスメモリを指している．
そして，このメモリは ```cudaFree``` API を使って解放する．
見てわかるように，ブロック内の全てのスレッドは，カーネル内のこのメモリにアクセスしている．

[ベクトル加算のコード](./code/02_vector_addition/vector_addition_gpu_thread_block.cu)は次のようにコンパイルできる．

```bash
$ nvcc -o vector_addition vector_addition_gpu_thread_block.cu
```

これが，簡単なグローバルメモリの使い方の例である．
次節では，最適なデータアクセスの方法を見ていく．


## 2.2.2 Coalesced versus uncoalesced global memory access
グローバルメモリを効果的に使うには，これまでは無視してきたが，
CUDA プログラミングモデルにおけるワープの概念を理解することが重要である．
ワープは SM 内でスレッドのスケジュールと実行を行うユニットである．
一旦ブロックが SM に割り当てられると，**ワープ (warp)** と呼ばれる32スレッドをまとめたユニットに分割される．
これが，CUDA プログラミングにおける基本的な実行単位になる．

ワープの概念をはっきりさせるために，簡単な例を見てみよう．
128スレッドを持つ2つのブロックが SM に割り当てられたとき，各ブロック内のワープ数は $128 / 32 = 4$ 個で，
SM 上のワープ数は合計で $4 \times 2 = 8$ 個である．
次の図は，GPU の SM 上でどのように CUDA のブロックが分割され，スケジューリングされるのかを表している，

<img src="image/IMG_0425.jpg">

ブロックとワープがどのように SM と そのコア上でスケージュルされるかは，Kepler や Pascal，そして最新の Volta 
アーキテクチャのように，アーキテクチャ依存で世代によって大きく異なる．
ここまで，スケジューリングについては無視できた．
使用可能な全てのワープの中で，次の実行される準備のできた演算子を持ったワープが実行対象に選ばれる．
CUDA のプログラムが実行されている GPU のスケジューリング・ポリシーに基づいて，実行されるワープが選ばれる．
CUDA は **Single Instruction Multiple Thread (SIMT)** モデルである．
つまり，ワープ内の全てのスレッドは，同時に同じ命令を読み出して実行している．
グローバルメモリから最適なアクセスを行うには，アクセスが結合して (coalesce) いる必要がある．
Coalesce と Uncoalesce の違いは次のとおりである．

- Coalesce global memory access : 隣接するメモリに逐次的にアクセス
- Uncoalesce global memory access : 隣接していないメモリに逐次的にアクセス

下図は2種類のアクセス方法の詳細な例である．
左側は，ワープからスレッドが隣接したデータにアクセスし，32個分を一括で取得し，1度のキャッシュミスで済む，
結合アクセスを表している．
右側は，ワープ内のスレッドがランダムに並んだデータにアクセスし，最悪の場合，32個を個別に呼び出す必要があり，
32回のキャッシュミスを起こしてしまう場合を表している．

<img src="image/IMG_0426.jpg">

さらにこの概念をよく理解するためには，データがどのようにグローバルメモリからキャッシュラインを通って
到達するか理解する必要がある．

### Senario 1 : ワープが4バイトの順番に並んだ32個のデータを要求したとき
アドレスは1つのキャッシュラインと32個一括の処理で済む．
バスの使用率は100%で，グローバルメモリからキャッシュに読み出したデータ全てが使われており，
帯域幅は全く無駄になっていない．
メモリのアクセスは次の図のように行われ，バスを最適に使いこなせている．

<img src="image/IMG_0427.jpg">>

### Senario 2 : ワープが4バイトのランダムに並んだ32個のデータを要求したとき
ワープが必要とするのは128バイトだが，実行中，1個ずつ32回読み出しを行っており，
バスを $32 \times 128$ バイトのデータが移動することになる．
次の図のように，バスの使用効率は1%未満になってしまい，非結合アクセスはバスの帯域幅を無駄にしていることがわかる．

<img src="image/IMG_0428.jpg">

ここまでの図を見てもわかるように，